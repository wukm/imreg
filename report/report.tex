\documentclass[10pt,twocolumn]{article}

\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{amsxtra, amscd, geometry, graphicx}
\usepackage{endnotes}
\usepackage{cancel}
\usepackage{alltt}
%\usepackage[all,cmtip]{xypic}
\usepackage{mathrsfs}
\usepackage{listings}
%\usepackage{subfigure}
%\usepackage[pdftex]{hyperref}
%\usepackage[dvips,bookmarks,bookmarksopen,backref,colorlinks,linkcolor={blue},citecolor={blue},urlcolor={blue}](hyperref}

% Makes the margin size a little smaller.
\geometry{letterpaper,margin=1.3in}
\usepackage{titlesec}
\usepackage[font=scriptsize, labelfont={sf,bf}, margin=1cm]{caption}
\usepackage{subcaption}

\titleformat*{\section}{\large\bfseries}
\titleformat*{\subsection}{\normalsize\bfseries}
% Possible font packages. Choose one and comment out the rest.
%\usepackage{times}%
%\usepackage{helvet}%
%\usepackage{palatino}%
%\usepackage{bookman}%

% These are italic.
%\theoremstyle{plain}
%\newtheorem{thm}{Theorem}
%\newtheorem*{thm*}{Theorem}
%\newtheorem{prop}{Proposition}
%\newtheorem*{prop*}{Proposition}
%\newtheorem{conj}{Conjecture}
%\newtheorem*{conj*}{Conjecture}
%\newtheorem{lem}{Lemma}
%  \makeatletter
%  \@addtoreset{lem}{thm}
%  \makeatother 
%\newtheorem*{lem*}{Lemma}
%\newtheorem{cor}{Corollary}
%  \makeatletter
%  \@addtoreset{cor}{thm}
%  \makeatother 
%\newtheorem*{cor*}{Corollary}
%
%%\newtheorem{lem}[thm]{Lemma}
%%\newtheorem{remark}[thm]{Remark}
%%\newtheorem{cor}[thm]{Corollary}
%%\newtheorem{prop}[thm]{Proposition}
%%\newtheorem{conj}[thm]{Conjecture}
%
%% These are normal (i.e. not italic).
%\theoremstyle{definition}
%\newtheorem*{ack*}{Acknowledgements}
%\newtheorem*{app*}{Application}
%\newtheorem*{apps*}{Applications}
%\newtheorem{defn}{Definition}
%\newtheorem*{defn*}{Definition}
%\newtheorem{eg}{Example}
%  \makeatletter
%  \@addtoreset{eg}{thm}
%  \makeatother 
%\newtheorem*{eg*}{Example}
%\newtheorem*{egs*}{Examples}
%\newtheorem{ex}{Exercise}
%\newtheorem*{ex*}{Exercise}
%\newtheorem*{quest*}{Question}
%\newtheorem{rem}{Remark}
%\newtheorem*{rem*}{Remark}
%\newtheorem{rems}{Remarks}
%\newtheorem*{rems*}{Remarks}
%\newtheorem{prob}{Problem}
%\newtheorem*{prob*}{Problem}
%\newtheorem*{soln*}{Solution}
%\newtheorem{soln}{Solution}
%
%
%% New Commands: Common Math Symbols
\providecommand{\R}{\mathbb{R}}%
%\providecommand{\N}{\mathbb{N}}%
%\providecommand{\Z}{{\mathbb{Z}}}%
%\providecommand{\sph}{\mathbb{S}}%
%\providecommand{\Q}{\mathbb{Q}}%
%\providecommand{\C}{{\mathbb{C}}}%
%\providecommand{\F}{\mathbb{F}}%
%\providecommand{\quat}{\mathbb{H}}%
%
%% New Commands: Operators
%\providecommand{\Gal}{\operatorname{Gal}}%
%\providecommand{\GL}{\operatorname{GL}}%
%\providecommand{\card}{\operatorname{card}}%
%\providecommand{\coker}{\operatorname{coker}}%
%\providecommand{\id}{\operatorname{id}}%
%\providecommand{\im}{\operatorname{im}}%
%\providecommand{\diam}{{\rm diam}}%
%\providecommand{\aut}{\operatorname{Aut}}%
%\providecommand{\inn}{\operatorname{Inn}}%
%\providecommand{\out}{{\rm Out}}%
%\providecommand{\End}{{\rm End}}%
%\providecommand{\rad}{{\rm Rad}}%
%\providecommand{\rk}{{\rm rank}}%
%\providecommand{\ord}{{\rm ord}}%
%\providecommand{\tor}{{\rm Tor}}%
%\providecommand{\comp}{{\text{ $\scriptstyle \circ$ }}}%
%\providecommand{\cl}[1]{\overline{#1}}%
%\providecommand{\tr}{{\sf trace}}%
%
%\renewcommand{\tilde}[1]{\widetilde{#1}}%
%\numberwithin{equation}{section}
%
%\renewcommand{\epsilon}{\varepsilon}
%
\newcommand*\rfrac[2]{{}^{#1}\!/_{#2}}

\newcommand*\mcol[1]{\overset{\uparrow}{\underset{\downarrow}{#1}}}
%
%% This makes the spacing between lines of font a little bigger.  I like the way it looks.
%\newcommand{\spacing}[1]{\renewcommand{\baselinestretch}{#1}\large\normalsize}
%\spacing{1.2}

% END PREAMBLE %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Graph Matching in $\R^2$}
\author{Lucas Wukmer}
\date{May 12, 2015}

\begin{document}

\maketitle
%\tableofcontents

\subsection*{Abstract}

A energy-maximizing method known as \emph{Spectral
Matching with Affine Constraint (SMAC)}  is utilized to identify a given subset
of points in the xy-plane which has been randomly transformed (rigid with
scaling) with their corresponding points in the original set. Our method views
these points as a subgraph of a larger attributed (complete) graph, whose edge
attributes are the euclidean distance between these points. 

\section{Introduction}

A well-known problem in computer vision is that of image registration (or
specifically point-set registration), which involves\footnote{Definitions may
vary.} finding a transformation that aligns a set of $n$ points in an
image $I$ with their $n$ counterparts in a different image $I'$. A related
question is whether or not we could find this transformation without knowing
exactly \emph{which} points map to the originals. In case where there are many
candidates (say $N \gg n$) for matching in the transformed image, this problem
is NP-complete{\cite{c01}: it would involve iterating through $P(N,n)$ possible
matchings and checking whether some particular quantity is conserved. This
direct approach becomes rather unreasonable for large values of N.

In some cases, it isn't necessary to think of images at all, but instead of
these two images being sets of points in $R^n$ (or some other metric space). We
focus first on a relatively simple case, where our sets are in $R^2$. Note that
in the discussion that follows, we do not concern outselves directly with
\emph{what} the transformation is that correctly registers the sets, but
instead the correct registration of the points themselves.

\section{Problem Overview \& Notation}

It this simplified version of the problem, we consider a set of $N$ random
points in the xy-plane, and some smaller subset of size $n$:

\begin{subequations}
  \begin{align}
    A := \left\{ (x,y) \in \R^2 \ | \  0 \leq x,y \leq 100 \right\} \\
    X \subset A \\
  N := |A|, \quad n := |X|
  \end{align}
\end{subequations}

\begin{figure*}
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{exact2}
    \caption{A system of 100 points in $R^2$. A random subset X (with $n=5$) is shown with
    its edges at its correct position in the larger system.}
  \end{subfigure} \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{transformed2}
    \caption{This X is then transformed (some random rescaling, translation, and rotation).
    Is it possible to reassociate these points with their correct points in A?}
  \end{subfigure}
\end{figure*}

Oftentimes it is more convenient to view these sets as matrices, with each
point in $R^2$ represented as a column, and we will use either notation freely
when no confusion arises: 

\begin{subequations}
  \begin{align}
    %A := \begin{bmatrix}
    %  \vdots  & \vdots & \  & \vdots \\
    %  a_1     & a_2    & \cdots   & a_N \\
    %  \vdots  & \vdots & \  & \vdots
    %\end{bmatrix} \in \R^{2\times N}\\
    A := \begin{bmatrix}
      \mcol{a_1} & \mcol{a_2} & \cdots   & \mcol{a_N}
    \end{bmatrix} \in \R^{2\times N}\\
    X := \begin{bmatrix}
      \mcol{x_1}    & \mcol{x_2}    & \cdots   & \mcol{x_n}
    \end{bmatrix} \in \R^{2\times n}\\
    N \geq n
  \end{align}
\end{subequations}

Of course, we consider it ``known'' the correct correspondence between X and A. That
is, we know the $n$ pairs $(i,j)$ such that $x_i = a_j$.

We then wish to apply a (reversible) affine transformation $T_{k, \theta, v}$
to the subsystem X as follows (given as homogeneous matrices for
simplification):

\begin{equation}
  \begin{aligned}
    \begin{bmatrix}X' \\ 1\end{bmatrix} &:=  T_{k, \theta, v} \begin{bmatrix}X \\ 1\end{bmatrix} \\
    &= \begin{bmatrix}
  k\cos\theta & -k\sin\theta & v_x \\
  k\sin\theta & k\cos\theta & v_y \\
  0             &   0 & 1
\end{bmatrix} \begin{bmatrix}X\\1\end{bmatrix} \\
k \in \R_+, \ \theta &\in [0, 2\pi),  \ v = (v_x, v_y) \in \R^2
\end{aligned}
\end{equation}

This equates to some arbitrary scaling, rotation, and translation of $X$.

Our goal is simply to find a matching between $X'$ and the original $A$ without
knowledge of the transformation $T$ itself. We denote any matching between $A$
and $X$ as a matrix $M \in \R^{N\times n}$ such that
\[
  M^T X \subset A 
\]

and in the case of the ``correct'' matching,
\begin{equation}
  MA = X
\end{equation}

where
\begin{equation}
  M_{ij} := \delta_{ij} \\
  = \left\{
    \begin{array}{lr}
    1 &: T(a_i) = x_j  \\
    0 &: T(a_i) \notin X
  \end{array}
  \right.
\end{equation}

Anyway.

\section{Methods}
We generate a point set $A$ size $N$ and subset $X$ size $n\leq N$ of uniformly
random floats in the range $[0,100]$. and transform each X with a uniformly
random $\theta \in [0, 2\pi)$ and $k\in [1,10]$ (our translation vector $v$ is
simply the one that makes $X'$ have a minimum $x$ and $y$ value of 0 each. See figure above.

We emultate the program introduced by Cour, et al.\cite{c02} and model
this problem as a graph matching problem and solve it via SMAC.

Specifically, we form two symmetric matrices
$D\in\R^{N \times N} , \ d\in\R^{n \times n}$ where
\[
  D_{ij} := || a_i - a_j ||_2 ,  \quad d_{ij} := || x_i - x_j ||_2
\]

We view these as edge attributes of the complete graph corresponding to A and its subgraph (which corresponds to X).

The similarity matrix $W \in \R^{Nn \times Nn}$ is then formed as follows:
\[
  W_{ii',jj'} := \frac{1}{1 + \left| \  d_{ij} - D_{i'j'}  \right| }
\]

That is, the $(ii',jj')$ element represents a ``score'' associated with a
matching containing both $(i,i'),(j,j')$. Note the ordering of elements along
either axis corresponds exactly to their order in the vectorized version of the
matching matrix $M$ above. The estimated matching matrix $M$ is then found as the
maximum of the following quadratic problem:

\[
  M \sim x_M := \mathsf{argmax} \left\{ x^T W x \ | \ Cx \preceq b \right\}
\]

where $x_M$ is the vectorized version of M. The condition $Cx\preceq b$ refers to a set of
affine conditions that guarantee each row in $M$ has at most one nonzero entry
(i.e. that the matching is one-to-one)\footnote{While simple to form, it
requires considerable \LaTeX \ gymnastics to notate properly, and so is left as an
exercise for the reader}.


We then calculate the leading eigenvector of the system $P_C W P_C$, with
$P_C \in \R^{Nn \times Nn}$. For the development and definitions of this material, we presently defer to \cite{c02}.

\subsection{Discretization of Matching Matrix} Our solution, a N-by-n matching
matrix $M$, is actually a relaxed solution, so we must discretize it. In doing
so, we wish to look for the best possible match in each column of M. Thus, we
'normalize' each column of M by dividing by the sum (i.e. each element
represents a percent of the total score across the column). Then our algorithm
proceeds as follows:

\begin{enumerate}
  \item divide each element by the sum of its column
  \item while M is not completely masked:
    \begin{enumerate}
        \item store the location of the maximal element in M
        \item mask the entire column and row it belongs to
    \end{enumerate}
  \item return M with zeros everwhere except for 1s at stored locations.
\end{enumerate} 
  

Thus we choose the best \emph{relative} match amongst each column.


\subsection{Scale factor search} In the case that X has been scaled in some
way (denote $X_{\mathrm{exact}} = kX$), our method is absolutely unusable. It depends highly on relative
distances between elements of the graphs, and our edge attributes $d$ for $X$
have little meaning if they are not given in the context of A. So we do a
linear search for $k$ by solving the system over a linear range and taking the
best.

Of course there is a clear upper and lower bound for what k can be in any given system:
\[
  \begin{aligned}
    k_{\mathrm{max}} &:= \frac{\mathsf{max}_{1\leq i,j\leq N}(D_{ij})}{\mathsf{max}_{1\leq i,j \leq n} d_{ij}} \\
  k_{\mathrm{min}} &:= \frac{\mathsf{min}_{1 \leq i,j \leq N }\left\{D_{ij} | D_{ij} > 0\right\}}{\mathsf{min}_{1 \leq i,j \leq n }\left\{d_{ij} | d_{ij} > 0\right\}}
\end{aligned}
\]

These boundaries simply state that k should never scale such that the largest
distance between points in X is greater than the largest distance between
points in A, and similarly that the smallest (nonzero)\footnote{Our matrices
$D, d$ are defined such that the diagonals are 0, which trivially represent the
'distance' between a point and itself.} distance in X should never be smaller
than the smallest (nonzero) distance in A.

We then divide the interval $[k_{\mathrm{min}} , k_{\mathrm{max}}]$ into equal
steps and simply compare the highest accuracy/energy at each.

\section{Results}

\subsection{Registration without Scaling}
We generated 200 random graphs with $N=25 n=4$ and attempted to do this problem with no scaling.
Over 200 trials, the average accuracy (correct number of nodes matched out of $n$) was $77.8\%$.

In the case that the subgraphs were clustered within the graph, the accuracy improved to $81.4\%$

\subsection{Registration with Scaling}

We ran 50 trials with equally spaced k values to estimate the scaling factor on a particular system (N=23, n=4) and were able to the correct scaling factor very closely:

\begin{verbatim} >> tests.scaling_problem()
...
best energy:          12.6520299071
corresp. accuracy:            100.0
estimated scale_factor 7.0064179968
real scale value: 6.962020393396437\end{verbatim}

Note that the correct energy is simply $n^2=16$ given our similarity matrix.
Other trials failed, which either were because the graph matching failed on the
system in general \emph{without} scaling, or because the mesh width was not
small enough to be close enough to the real maximum energy. As shown in
\textbf{Figure 2}, the energy in general is not convex, and does not correspond
predictably to the pointwise accuracy of the match. However, it is imagined
that the correct scaling \emph{could theoretically} be found by repeating a
linear search for k on a reduced area with a smaller step size, probing areas
where the accuracy or energy was reasonably high. This is not a very reasonable
solution however, as our method for finding $k$ would already be prohibitively
expensive computationally for large systems.

\begin{figure}[t]
    \centering
    \includegraphics[width=.5\textwidth]{scalesearch}
    \caption{A successful matching search for a small system $(N=23,n=4)$ with 50 discrete points between
      $[k_{\mathsf{min}}, k_{\mathsf{max}}]$. $100\%$ accuracy was achieved
    with $k=7.0064$, which was closest to the real scale factor $k_0=6.9620$}
    %\label{fig:stuff}
\end{figure}

\section{Conclusion}

There are several further directions in which it is likely possible to improve our model. The following is a wish-list of sorts:

\begin{itemize}
  \item Get a rough estimate of $\mathcal{O}(\mathrm{SMAC})$ vs $\mathcal{O}(\mathrm{brute force})$ on smaller systems
  \item Improve upon unscaled graph matching and understand failing cases
  \item A faster method, if possible, for finding $k$ 
  \item To what extent this problem can still be solved when a small amount of noise/perturbation is added to X: $X + \varepsilon$
  \item Can this problem be done if there are a few points in $X$ which do not correspond to points in $A$ (like outliers)
\end{itemize}


\section{Special Thanks}

\begin{itemize}
  \item James von Brecht for his continued insight and patience related to this project.
\end{itemize}

\section{Special No Thanks}
\begin{itemize}
  \item My computer fan for blowing out
  \item Mathworks for not offering a 32bit binary of MATLAB
  \item matplotlib
\end{itemize}

\begin{thebibliography}{9}
\bibitem{c01}
  James von Brecht, personal communication
\bibitem{c02}
  Cour, Timothee, Praveen Srinivasan, and Jianbo Shi.
  \emph{"Balanced graph matching."}
  Advances in Neural Information Processing Systems 19 (2007): 313.
\end{thebibliography}
\end{document}
